
This is an outline for Lecture 2


# Motivation

* What is the problem?
  - We ran an experiment with two treatments, to see if they have any effect on a downstream response
  - Are the two treatments different? How sure should we be?
  - Examples: Treatment and control in a drug trial (outcome = cholesterol lev
el), alternative manufacturing protocols (outcome = number of defects per 1000 units), two designs of a web page (outcome = number of users clicking), ...
* What you should be able to do
  - Recognize a comparison experiment, and know which type of t-test (equal
    variance, unequal variance, or paired) is appropriate
  - Construct a confidence interval for the difference between treatments, which
    accounts for the equal / unequal variance or paired structure in the data

# Example

* Look at a specific dataset: cement
  - two treatments: two different formulas for making concrete
  - response: how strong is the concrete?
* show all the numbers, from read.table
* display the boxplot

# Formalization

* two groups of numbers, y[1, 1], y[1, 2], ... and y[2, 1], y[2, 2], ...
* draw two ragged arrays

# Necessary for test: Model Setup

y[i, j] = mu[i] + eps[i, j]
where eps[i, j] ~ N(0, sigma[j]^2)
test is whether mu[1] = mu[2]
vs. one of three alternatives,
mu[1] != mu[2]
mu[1] > mu[2]
mu[1] < mu[2]

## Give some pictures
ragged array maps onto two lines, with errors coming from eps
ragged array maps onto two normal distributions
* note, model exists apart from data. if you ran the experiment again, you would
  get a different histogram.

## Test when sigma[1] = sigma[2]
* Estimate mu[1] and mu[2], and measure the difference in standardized units
* Formally,
Z[0] = (\bar{y[1]} - \bar{y[2]}) / sd(\bar{y[1]} - \bar{y[2]})
= (\bar{y[1]} - ...) / sigma * sqrt{1 / n1 + 1/ n2}

Assuming mu[1] = mu[2], the "reference distribution" for Z[0] is
Z[0] ~ N(0, 1)
You can use this to gauge whether the difference in observed means is large.
"Under the null, the probably that I would see such a large difference is ...
It's always possible, just improbably"

### Estimating sigma^2

The above derivation treated sigma as a fixed, known constant. In reality, it
must be estimated as well. A natural estimate is

s_{p}^2 = [(n1 - 1) s[1]^2 + (n2 - 1) s[2] ^ 2] / (n1 + n2 - 2)
interpretation: look at the sum of squares from each of the estimated means, and divide

The reference distribution needs to account for the extra randomness coming from
estimating sigma^2. The updated reference is

t[0] ~ t(n1 + n2 - 2)

Draw a picture of when you would reject or not


### Recipe

Testing:
when you reject each of the three hyp.

p-values:
in each of the three cases


confidence intervals:
in each of the three cases

# Interpretation via Linear Regression

the model y[ij] = mu[j] + eps[i, j] is equivalent to

y[i] = beta[0] + beta[1] D[i] + eps[i]

where mu[2] = beta[0], mu[1] = beta[0] + beta[1].

Draw a picture of the points unwrapped along a long index 1, ..., n1, n1 + 1, ... n2

# Code example

# Test when sigma[1] != sigma[2]

What happens if the variances in the two groups are not the same?

* The variance of the difference is changed, but can still obtain a N(0, 1)
  reference distribution with appropriate standardization,

z[0] = (y[1] - y[2]) / sqrt(sigma[1]^2 / n1 + sigma[2]^2 / n2) ~ N(0, 1)

* If you don't know sigma1 and sigma2, you need to estimate them. The estimator
  is different from our previous one, and the resulting reference distribution
  also changes,

t[0] = (y[1] - ...) / (sqrt s1^2 / n1 + s2^2 / n2) ~ t(nu)

where nu is the integer closest to (s1^2 / n1 + s2^2 / n2) ^ 2 / [(s1^2 / n1) ^ 2 / (n1 - 1) + (s2^2 / n2) ^ 2 / (n2 - 1)]

(not an obvious derivation). It's sometimes called welch's t.

recipe:
one sided, left

test rejection
p-value
confidence interval

one sided, right
two sided

Connection to regression

Model is formally equivalent to 

y[i] = beta0 + beta1 D1 + eps[i]

where eps[i] ~ sigma1^2 for Di = 1, sigma2^2 for Di = 0.

## example in R

# Which test to use when?

You need to check whether sigma1^2 = sigma2^2.

Estimate each using s1^2 and s2^2. If they are very different from one another,
reject this hypothesis.

It turns out that the reference is available in closed form,

F0 = s1^2 / s2^2 ~ F(n1 - 1, n2 - 1)
1 / F0 ~ F(n2 - 1, n1 - 1)

So we reject if F0 is too large or too small. (alpha / 2 in each test, draw picture)

Can also build a confidence interval for the ratio.

# Common Alternative: Paired Data

What happens when you give both treatments to each experimental unit?
These experimental units vary in ways that are irrelevant. By pairing, you are
"blocking" out the nuisance factor.

Example with shoes.

Data example: tips measuring metal hardness.


## Model setting

Like before,

y[i, j] = mu[i] + eps[i, j]

But now, the eps[i, j] are correlated for each fixed j. You're likely to be high
on both, or low on both.

It turns out that, under the null, the statistic

\bar{d} = average(y1j - y2j) / (sigmad / sqrt(n)) ~ N(0, 1)

so it makes a good reference distribution.

As before, from a reference distribution, we can define a hypothesis test, a
p-value, and a confidence interval, for each of the three alternative
hypotheses.

R example

* Show code and give people link to the binder
