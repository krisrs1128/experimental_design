---
title: Simple Comparative Experiments
date: September 10, 2020
subtitle: Chapter 2.4 - 2.6
output:
  html_document:
    css: style.css
---
&nbsp;

# Setup and Motivation

What is the problem?

* Imagine that we ran an experiment under two treatment conditions, to see if
  they had different effects on a downstream, continuous response.
* How can you quantify the difference in effects (if any) between treatments?

&nbsp;

Common Examples

* Drug and placebo in a pharmaceutical trial (outcome = cholesterol level)
* Alternative manufacturing process (outcome = number of defects per 1000 units)
* Two designs of a web page (outcome = number of users who donate)

&nbsp;

_Learning Objective_: After studying this section, you should be able to,

* Recognize a comparative experiment in the while, and know which type of test
  ($z$-test, $t$-test with equal variances, $t$-test with unequal variances, or
  paired $t$-test) is appropriate
* For each of these tests, be able to perform calculations needed to provide
  hypothesis tests, $p$-values, and confidence intervals.

## Example

A company has run an experiment comparing the strength of concrete made using
two different mixes of chemicals. The question is, do the mixes have the same
strength? If you ran the experiment again, how sure can you be that your
conclusion will stand?

Here are all `${x}` observations,

```{r}
library("readr")
library("reshape2")
library("ggplot2")
theme_set(theme_bw() + theme(panel.grid = element_blank()))

cement <- read_csv("cement.txt")
cement

# data needs to be in "tall" format for ggplot2
mcement <- melt(cement, variable.name = "Formula", value.name = "Strength")
ggplot(mcement) +
  geom_point(aes(x = Formula, y = Strength))
```

[Run the example yourself](https://hub-binder.mybinder.ovh/user/krisrs1128-experimental_design-4oeubz7v/rstudio/?token=vlJ2WrvsQzOfghgLCxFX-w)


## Formalization

Now we'll make this more formal, building the mathematical abstraction that we
can apply in many situations.

Arrange the data from the two treatments into two rows,

$$
y_{11}, y_{12}, \dots, y_{1n_{1}} \\
y_{21}, y_{22}, \dots, y_{2n_{2}},
$$

so that $y_{ij}$ corresponds to the $j^{th}$ observation in the $i^{th}$
treatment. $n_{1}$ and $n_{2}$ denote the number of samples given each
treatment.

In order to make any sorts of probabilistic statements about what would happen
if we ran the experiment again, we need to assume a model that generated the
data. In this set of notes, we'll consider,

$$
y_{ij} = \mu_{i} + \epsilon_{ij} \\
\epsilon_{ij} \overset{i.i.d.}{\sim} \mathcal{N}\left(0, \sigma_{j}^{2}\right),
$$

a Gaussian model with potentially different means and variances in each
treatment $i$. Different types of tests correspond to different constraints on
the possible values of the $\mu_{i}$'s and $\sigma_{i}$'s.

![](figures/comparative_setup.png)
&nbsp;


For hypothesis testing, we're interested in testing the null,

$$
H_{0}: \mu_{1} = \mu_{2}
$$

against one of three alternatives,

$$
H_{a1}: \mu_{1} \neq \mu_{2} \\
H_{a2}: \mu_{1} > \mu_{2} \\
H_{a3}: \mu_{1} < \mu_{2} \\
$$

# Tests assuming $\sigma^2_{1} = \sigma^2_{2}$

It's natural to estimate $\mu_{1}$ and $\mu_{2}$ by the corresponding sample
means, $\bar{y}_{1}$ and $\bar{y}_{2}$, respectively. If we want to see whether
$\mu_{1} = \mu_{2}$, we can then consider the size of $\bar{y}_{1} -
\bar{y}_{2}$: values that are very positive or very negative provide evidence
against the null hypothesis. But, how large is large?

If we knew that $\sigma^2_{1} = \sigma^2_{2} := \sigma^2$, then observe that

$$
\text{Var}\left(\bar{y}_{1} - \bar{y}_{2}\right) = \text{Var}\left(\bar{y}_{1}\right) + \text{Var}\left(\bar{y}_{2}\right) \\
= \frac{\sigma^2}{n_1} + \frac{\sigma^2}{n_2} \\
= \sigma^2 \left(\frac{1}{n_1} + \frac{1}{n_2}\right)
$$

and since under the null, the expected difference is 0, we have (why[^1]?)
$$
Z:= \frac{\bar{y}_{1} - \bar{y}_{2}}{\sigma \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim \mathcal{N}\left(0, 1\right).
$$

This is a very useful result, because it gives us a *reference
distribution* for our test statistic. If we want to tell whether a observed
value of the statistic is consistent with the null hypothesis, we can see
whether or not it falls into the bulk of the reference distribution.

Which of these test statistics is consistent with the $\mathcal{N}\left(0,
1\right)$ null?

<img src="figures/reference_distn_moderate.png" width="300"/>
<img src="figures/reference_distn_extreme.png" width="300"/>
<img src="figures/reference_distn_null.png" width="300"/>

## Estimating $\sigma^2$

The above derivation treated sigma as a fixed, known constant. In reality, it
must be estimated as well. We can use,

$$
s_{p}^2 = \frac{1}{n_1 + n_2 - 2}\left[\sum_{j = 1}^{n_{1}} \left(y_{1j} - \bar{y}_1\right)^2 + \sum_{j = 1}^{n_{2}} \left(y_{2j} - \bar{y}_{2}\right)^2\right] \\
= \frac{1}{n_1 + n_2} \left[\left(n_1 - 1\right)s_{1}^2 + \left(n_2 - 1\right)s_{2}^{2}\right]
$$

<img src="figures/pooled_standard_error.png" width="300"/>

which looks at the squared deviations from the mean within each group. It's the
two-group generalization of the usual standard error estimate, where the groups
have the same variances but not necessarily the same means.

The reference distribution needs to account for the extra randomness coming from
estimating $\sigma^2$. The updated reference is

$$
T = \frac{\bar{y}_1 - \bar{y}_2}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t(n_1 + n_2 - 2)
$$

which has slightly heavier tails than the normal distribution.

## Recipe

In either the known or estimated $\sigma^2$ case, we can now use our test
statistics and their reference distributions to (1) perform hypothesis tests,
(2) compute of $p$-values, and (3) build confidence intervals.

**Hypothesis Testing**:
* To perform a level $\alpha$ test of the null hypothesis, we see whether the
  test statistic lies in the rejection rejection specified by the reference
  distribution. The rejection regions for $H_{a1}, H_{a2}$, or $H_{a3}$ are
  sketched below.

Rejection region for $H_{a1}$:
<img src="figures/h_a1.png" width="300"/>

Rejection region for $H_{a2}$:
<img src="figures/h_a2.png" width="200"/>

Rejection region for $H_{a3}$:
<img src="figures/h_a3.png" width="200"/>
  
* The point is that the rejection region should include values the statistic
would take on under the alternative. But it should also be small enough so that,
under the null, you would only accidentally reject $\alpha$% of the time.
  
**$p$-values**

* Hypothesis testing can be very black-or-white: you either reject a hypothesis
  or you don't.
* $p$-values can give shades of gray. They describe the magnitude of the
  discrepancy from the null hypothesis.
* To compute a $p$-value, compute the area under the reference distribution that
  is more extreme than the observed test statistic.
  
Example $p$-value for $H_{a2}$.
<img src="figures/one_sided_pval.png" width="200"/>
  
Example $p$-value for $H_{a1}$.
<img src="figures/two_sided_pval.png" width="200"/>

**Confidence Intervals**

* A confidence interval gives the set of differences $\mu_1 - \mu_2$ for which
  you would not reject the null hypothesis.
* They can be computed by inverting the equation that comes from setting the
  test statistic equal to the test rejection thresholds. For example, when
  $\sigma^2$ is known, you would use a Normal reference distribution, giving
  equations,
  
$$
\frac{\bar{y}_{1} - \bar{y}_{2}}{\sigma} = z^{\ast}\left(\frac{\alpha}{2}\right) \\
\frac{\bar{y}_{1} - \bar{y}_{2}}{\sigma} = z^{\ast}\left(1 - \frac{\alpha}{2}\right) \\
$$

and hence the intervals,

$$
\left[\left(\bar{y}_1 - \bar{y}_2\right) - \sigma z^{\ast}\left(1 - \alpha\right), \left(\bar{y}_1 - \bar{y}_2\right) + \sigma z^{\ast}\left(1 - \alpha\right)\right].
$$

Q: What is the corresponding interval when $\sigma^2$ is not known?

## Interpretation via Linear Regression

Earlier, we assumed the model,

$$
y_{ij} = \mu_{i} + \epsilon_{ij} \\
\epsilon_{ij} \overset{i.i.d.}{\sim} \mathcal{N}\left(0, \sigma_{j}^{2}\right),
$$
where $i = 1, 2$ indexed the two treatments and $j$ indexed the samples within
each treatment.

In the case that $\sigma_{1}^2 = \sigma_{2}^2 = \sigma^2$, this can be rewritten
as,

$$
y_{i} = \beta_{0} + \beta_{1}D_{i} + \epsilon_{i} \\
\epsilon_{i} \overset{i.i.d.}{\sim} \mathcal{N}\left(0, \sigma^{2}\right),
$$

where $i = 1, 2, \dots, n_{1}, n_{1} + 1, \dots, n_{1} + n_{2}$ indexes all
samples, from both treatments and

\begin{equation}
D_{i} = \begin{cases} 
1 & \text{ if sample } i \text{ is from treatment 2} \\
0 & \text{ if sample } i \text{ is from treatment 1}.
\end{cases}
\end{equation}

In this model, the mean under treatment 1 is $\beta_{0}$, and the mean under
treatment 2 is $\beta_{0} + \beta_{1}$. These play the role of $\mu_{1}$ and
$\mu_{2}$ from before. If the two means are equal, then $\beta_{1} = 0$.


<img src="figures/linear_reg.png" width="500"/>

Q's: What would this look like if there were no treatment effect? What would
this look like if $n_{1} \gg n_{2}$.

## Code example

From our plot above, it looks like the unmodified cement is stronger. But it's
easy to be tricked into seeing patterns which are explainable by pure
randomness, so a test would still be worthwhile. We'll assume that the variance
$\sigma^2$ is the same in both groups, but will still need to estimate. So, we
will use a $t$-test. A priori, we didn't have a guess about whether one formula
was stronger than the other, so we'll test against the two-sided alternative
$H_{a1}$.

```{r}
test_result <- t.test(cement$Modified, cement$Unmodified, var.equal = TRUE, alternative="two.sided", conf.level=0.95)
test_result
```

The result of the test suggest that there's some evidence that the formulas
don't have equal strengths.

Q: How would you test against the alternative $H_{a2}$ that the unmodified
concrete is stronger than the modified one?

# Tests assuming $\sigma^2_{1} \neq \sigma^2_{2}$

What happens if the variances in the two groups are not the same?

<img src="figures/different_sigmas.png" width="300"/>

Notice that, by independence within and between groups,

$$
\text{Var}\left(\bar{y}_1 - \bar{y}_2\right) = \text{Var}\left(\bar{y}_1\right) +  \text{Var}\left(\bar{y}_2\right) \ \\
= \frac{\sigma_1^2}{n_1} + \frac{\sigma_{2}^2}{n_2}.
$$

Therefore, if both $\sigma_1$ and $\sigma_2$ are known, we can obtain a
$\mathcal{N}\left(0, 1\right)$ reference distribution by standardizing
appropriately,

$$
Z_0 = \frac{\bar{y}_1 - \bar{y}_2}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}} \sim \mathcal{N}\left(0, 1\right).
$$

If we don't know $\sigma_1$ or $\sigma_2$, then we have to estimate them. We can
use the same estimators $s_1$ and $s_2$ as before. But this time, we don't
attempt to pool them into $s_p$. It turns out that this leads to an alternative
reference $t$-distribution,

$$
T_0 = \frac{\bar{y}_1 - \bar{y}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} \sim t\left(\nu\right)
$$

where

$$
\nu = \text{Round}[ \frac{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}{\frac{\left(\frac{s_1^2}{n_1}\right) ^ 2}{n_1 - 1} + \frac{\left(\frac{s_2^2}{n_2}\right) ^ 2}{n_2 - 1}}\right],
$$

This is far from obvious; fortunately, most software will compute this $\nu$
automatically in the background.

With these reference distributions, we can obtain tests, $p$-values, and
confidence intervals, similarly as above. For example, a confidence interval for
the difference in means (when $\sigma_1^2 \neq \sigma_2^2$ and both must be
estimated) has the form,

$$
\left[\left(\bar{y}_1 - \bar{y}_2\right) - t^\ast\left(1 - \frac{\alpha}{2}; \nu\right)\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}, 
\left(\bar{y}_1 - \bar{y}_2\right) - t^\ast\left(1 - \frac{\alpha}{2}; \nu\right)\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}\right]
$$

## Interpretation via Linear Regression

As in the equal variance case, we can interpret the assumed model using the
language of regression. The only thing that changes is that the variances are no
longer equal in each treatment group.

\begin{equation}
y_{i} = \beta_{0} + \beta_{1}D_{i} + \epsilon_{i} \\
\epsilon_{i} \overset{\text{indep}}{\sim} \begin{cases}
\mathcal{N}\left(0, \sigma_1^{2}\right) & \text{ when } D_i = 0 \\
\mathcal{N}\left(0, \sigma_2^{2}\right) & \text{ when } D_i = 1,
\end{cases}.
\end{equation}

## Code Example

To implement the test when variances are not equal, we simply need to set the
`var.equal` parameter to `FALSE`. The estimators and degrees of freedom are
calculated in the background.

```{r}
test_result <- t.test(cement$Modified, cement$Unmodified, var.equal = FALSE, alternative="two.sided", conf.level=0.95)
test_result
```

# Paired Data: A first look at blocking

Sometimes, there is variation in our experimental units that isn't coming from
our treatments, and which can make it harder to recognize real treatment
effects. This type of variation is called _nuisance_ variation. This was the
case in the shoes example from the first lecture: a lot of the variation in the
wear of the shoes came from some people walking more than others, not
necessarily the type of shoe sole used.

One common solution is to use pairing. The idea is to give both treatments to
each experimental unit. Then, even if the experimental units vary substantially,
you might be able to recognize changes within each unit. By pairing, you are
"blocking out" the nuisance variation experimental experimental units.

Our running example will be a study on the hardness of etching tips. There are
two types of tips (two treatments), and we want to know whether they have equal
hardness. The usual test for this requires etching into a sheet of metal (called
a metal specimen). However, it sometimes happens that certain sheets are harder
to etch into than others -- this is nuisance variation between sheets.

To bypass this nuisance variation, the idea is to test both tips on each of the
metal sheets. The difference in etching ability per sheet will tell us about the
relative hardness of the tips. Here is the data,

```{r}
etch <- read_csv("etch.txt")

m_etch <- etch %>%
  melt(id.vars = "Specimen", variable.name = "Tip")
m_etch$Specimen <- factor(m_etch$Specimen, order(rowMeans(etch[, -1])))

ggplot(m_etch) +
  geom_point(aes(x = Tip, y = value)) +
  geom_line(aes(x = Tip, y = value, group = Specimen)) +
  facet_wrap(~Specimen)
```

Each panel in the plot is one metal specimen. The reading for Tip 1 is given on
the left of each line segment, that for Tip 2 is on the right. Upward sloping
lines are those where Tip 2 is stronger than Tip 1, and vice versa for the
downwards sloping lines. The specimens are sorted from those which had low
readings for both tips to those which had high readings for both. Note that the
variation from one tip to another is smaller than the variation across the
different sheets of metal -- if we didn't pair, the nuisance variation would
potentially drown out interesting variation.

Indeed, if you ignored the pairing information, this is how the difference
between the two tips would look.

```{r}
ggplot(m_etch) +
  geom_boxplot(aes(x = Tip, y = value))
```

## Inference

To perform inference, we'll want a data generating mechanism in the paired data
case. The crucial change is that the errors are no longer independent within a
batch. For example, the correlation between errors means that if a specimen has
a lower reading for one tip, then it is likely to have a lower reading for the
other tip as well.

Formally, we suppose,

$$
y_{ij} = \mu_{i} + \epsilon_{ij} \\
\epsilon_{i} \overset{i.i.d.}{\sim} \mathcal{N}\left(\begin{pmatrix}0 \\ 0\end{pmatrix}, \begin{pmatrix} \sigma_1^2 & \rho\sigma_1\sigma_2 \\ \rho\sigma_1\sigma_2 & \sigma_2^2 \end{pmatrix}\right).
$$

Let's define a statistic that averages the pairwise differences,

\begin{equation}
\bar{d} := \frac{1}{n}\sum_{i = 1}^{n} \left(y_{i1} - y_{i2}\right).
\end{equation}

Intuitively, when $\left| \bar{d}\right|$ is large, we will reject the null. To
get a sense of how large is large, we'll want a reference distribution. To this
end, notice that

\begin{equation}
\text{Var}\left(y_{i1} - y_{i2}\right) &= \text{Var}\left(y_{i1}\right) + \text{Var}\left(y_{i2}\right) - 2\text{Cov}\left(y_{i1}, y_{i2}\right) \\
&= \sigma_1^2 + \sigma_2^2 - 2\rho\sigma_1\sigma_2.
\end{equation}

So, if $\sigma_1, \sigma_2$ and $\rho$ are all known, then under the null hypothesis,
\begin{equation}
\frac{\bar{d}}{\sqrt{\frac{\sigma_1^2 + \sigma_2^2 - 2\rho\sigma_1\sigma_2}{n}}} &\sim \mathcal{N}\left(0, 1\right),
\end{equation}

and so we can use a standard normal as the reference for all testing,
$p$-values, and confidence intervals.

When the variance and covariances are unknown, we have to estimate them. Once we
plug-in these estimates into our test statistic, the resulting reference
distribution is a $t$-distribution, though the degrees of freedom have changed
from before,

\begin{equation}
\frac{\bar{d}}{\sqrt{\frac{s_1^2 + s_2^2 - 2s_{12}}{n}}} &\sim t\left(n - 1\right).
\end{equation}

# Code Example

We can implement the paired $t$-test using the same `t.test` function that we
have been using all along, but we need to set the `paired` argument to `TRUE`.

```{r}
test_result <- t.test(etch$Tip1, etch$Tip2, alternative="two.sided", paired=TRUE)
test_result
```

The result suggests that whatever differences we noticed between the tips could
have been due to chance alone.

Earlier, I made a general statement about true treatment effects being "drowned
out" by nuisance variation when pairing is not used. For illustration, let's
consider a made up dataset where there is in fact a small treatment effect, but
the variation across experimental units would make that impossible to have
noticed.

```{r}
mus <- c(0, 0.2)
Sigma <- matrix(c(1, 0.9, 0.9, 1), 2)

y <- MASS::mvrnorm(10, mus, Sigma)
m_y <- melt(y, varnames = c("Specimen", "Tip")) %>%
  mutate(
    Specimen = factor(Specimen, levels = order(rowMeans(y))),
    Tip = as.factor(Tip)
  )

ggplot(m_y) +
  geom_boxplot(aes(x = Tip, y = value))

ggplot(m_y) +
  geom_point(aes(x = Tip, y = value)) +
  geom_line(aes(x = Tip, y = value, group = Specimen)) +
  facet_wrap(~Specimen)

# much more significant when pairing than otherwise
t.test(y[, 1], y[, 2])
t.test(y[, 1], y[, 2], paired = TRUE)
```


[^1]: Hint: Any linear combination of Gaussians is still Gaussian, and these
distributions are completely determined by their means and variances. Also, for
any fixed constant $a$, if $X \sim \mathcal{N}\left(0, \tau^2\right)$, then $a X
\sim \mathcal{N}\left(0, a^2 \tau^2\right)$.
